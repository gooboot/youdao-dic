#!/usr/bin/env python3
import json
import os
import math

def split_unique_words():
    """
    å°†unique_words.jsonä¸­çš„å•è¯æ¯1000ä¸ªåˆ†å‰²åˆ°å¯¹åº”çš„jsonæ–‡ä»¶ä¸­
    ä¿æŒæ¯ä¸ªå•è¯çš„JSONç»“æ„ä¸€è‡´
    """

    # æ–‡ä»¶è·¯å¾„
    input_file = os.path.expanduser('~/Desktop/myNote/unique_words.json')
    output_dir = os.path.expanduser('~/Desktop/myNote/split_words/')

    print("ğŸ” è¯»å–å”¯ä¸€å•è¯æ–‡ä»¶...")
    print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {input_file}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_dir}")
    print("=" * 50)

    try:
        # è¯»å–åŸå§‹æ•°æ®
        with open(input_file, 'r', encoding='utf-8') as f:
            words_data = json.load(f)

        total_words = len(words_data)
        words_per_file = 1000
        total_files = math.ceil(total_words / words_per_file)

        print(f"ğŸ“Š æ€»å•è¯æ•°é‡: {total_words}")
        print(f"ğŸ“‹ æ¯æ–‡ä»¶å•è¯æ•°: {words_per_file}")
        print(f"ğŸ“„ é¢„è®¡ç”Ÿæˆæ–‡ä»¶æ•°: {total_files}")
        print("=" * 50)

        # åˆ†å‰²æ•°æ®å¹¶ä¿å­˜
        for file_index in range(total_files):
            start_index = file_index * words_per_file
            end_index = min((file_index + 1) * words_per_file, total_words)

            # è·å–å½“å‰åˆ†ç‰‡çš„æ•°æ®
            current_batch = words_data[start_index:end_index]

            # é‡æ–°åˆ†é…wordRankï¼ˆä»1å¼€å§‹ï¼‰
            for i, word_obj in enumerate(current_batch, 1):
                word_obj['wordRank'] = i

            # ç”Ÿæˆæ–‡ä»¶å
            output_filename = f"words_{file_index + 1:03d}.json"
            output_path = os.path.join(output_dir, output_filename)

            # ä¿å­˜æ–‡ä»¶
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(current_batch, f, ensure_ascii=False, indent=2)

            # æ˜¾ç¤ºè¿›åº¦
            actual_count = len(current_batch)
            progress = ((file_index + 1) / total_files) * 100
            print(f"âœ… [{file_index + 1:3d}/{total_files}] {output_filename} - {actual_count} ä¸ªå•è¯ ({progress:.1f}%)")

        print("=" * 50)
        print("âœ¨ åˆ†å‰²å®Œæˆ!")
        print(f"ğŸ“ ç”Ÿæˆæ–‡ä»¶æ•°é‡: {total_files}")
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_dir}")

        # éªŒè¯ç»“æœ
        print("\nğŸ” éªŒè¯ç»“æœ...")
        total_split_words = 0
        for file_index in range(total_files):
            filename = f"words_{file_index + 1:03d}.json"
            filepath = os.path.join(output_dir, filename)
            if os.path.exists(filepath):
                with open(filepath, 'r', encoding='utf-8') as f:
                    split_data = json.load(f)
                    total_split_words += len(split_data)

        print(f"ğŸ“Š åŸå§‹å•è¯æ•°: {total_words}")
        print(f"ğŸ“Š åˆ†å‰²åæ€»æ•°: {total_split_words}")
        if total_words == total_split_words:
            print("âœ… éªŒè¯æˆåŠŸ: æ•°é‡ä¸€è‡´!")
        else:
            print("âŒ éªŒè¯å¤±è´¥: æ•°é‡ä¸ä¸€è‡´!")

        # æ˜¾ç¤ºç¤ºä¾‹æ–‡ä»¶ä¿¡æ¯
        if total_files > 0:
            first_file = os.path.join(output_dir, "words_001.json")
            last_file = os.path.join(output_dir, f"words_{total_files:03d}.json")

            print(f"\nğŸ“„ ç¤ºä¾‹æ–‡ä»¶ä¿¡æ¯:")
            with open(first_file, 'r', encoding='utf-8') as f:
                first_data = json.load(f)
                print(f"   words_001.json: {len(first_data)} ä¸ªå•è¯")
                print(f"   é¦–ä¸ªå•è¯: {first_data[0]['headWord']}")

            with open(last_file, 'r', encoding='utf-8') as f:
                last_data = json.load(f)
                print(f"   words_{total_files:03d}.json: {len(last_data)} ä¸ªå•è¯")
                print(f"   æœ«ä¸ªå•è¯: {last_data[-1]['headWord']}")

    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {input_file}")
    except json.JSONDecodeError as e:
        print(f"âŒ é”™è¯¯: JSONè§£æå¤±è´¥: {e}")
    except Exception as e:
        print(f"âŒ é”™è¯¯: å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜: {e}")

if __name__ == "__main__":
    split_unique_words()